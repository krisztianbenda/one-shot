{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy.random as rng\n",
    "import numpy as np\n",
    "import os\n",
    "import dill as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def W_init(shape, name=None):\n",
    "    values = rng.normal(loc=0, scale=1e-2,size=shape)\n",
    "    return K.variable(values, name=name)\n",
    "def b_init(shape, name=None):\n",
    "    values=rng.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_best_acc(best_acc_str):\n",
    "    with open('accuracies.txt') as f:\n",
    "        content = f.readlines()\n",
    "    best_acc = 0\n",
    "    for line in content:\n",
    "        if line.find(best_acc_str) >= 0:\n",
    "            cont = line.split(' ')\n",
    "            best_acc = int(cont[1])\n",
    "    return best_acc\n",
    "\n",
    "def save_best_acc(best_acc_str, best_acc):\n",
    "    with open('accuracies.txt', 'r') as f :\n",
    "        content = f.readlines()\n",
    "\n",
    "    exists = False\n",
    "    for i, s in enumerate(content):\n",
    "        if content[i].find(best_acc_str) >= 0:\n",
    "            content[i] = best_acc_str + ': ' + str(best_acc)\n",
    "            exists = True\n",
    "\n",
    "    if not exists:\n",
    "        content.append(best_acc_str + ': ' + str(best_acc) + '\\n')\n",
    "\n",
    "    with open('accuracies.txt', 'w') as f:\n",
    "        f.writelines(content)\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbenda/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:25: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/kbenda/anaconda2/lib/python2.7/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/kbenda/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38951745"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape=(105,105,1)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64,(10,10),activation='relu',input_shape=input_shape,\n",
    "                   kernel_initializer=W_init,kernel_regularizer=l2(2e-4)))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(128,(7,7),activation='relu',\n",
    "                   kernel_regularizer=l2(2e-4),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(128,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(256,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "L1_dist = lambda x: K.abs(x[0]-x[1])\n",
    "both = merge([encoded_l,encoded_r], mode = L1_dist, output_shape = lambda x: x[0])\n",
    "prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(both)\n",
    "siamese_net = Model(input=[left_input,right_input],output=prediction)\n",
    "\n",
    "optimizer = Adam(0.00006)\n",
    "siamese_net.compile(loss='binary_crossentropy',optimizer=optimizer)\n",
    "\n",
    "siamese_net.count_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alphabet_of_the_Magi': [185, 204], 'Cyrillic': [440, 472], 'Gujarati': [599, 646], 'Japanese_(katakana)': [83, 129], 'Japanese_(hiragana)': [647, 698], 'Sanskrit': [221, 262], 'Korean': [473, 512], 'Mkhedruli_(Georgian)': [318, 358], 'Ojibwe_(Canadian_Aboriginal_Syllabics)': [585, 598], 'Latin': [559, 584], 'Early_Aramaic': [699, 720], 'Grantha': [881, 923], 'Asomtavruli_(Georgian)': [924, 963], 'Futurama': [385, 410], 'Bengali': [513, 558], 'Inuktitut_(Canadian_Aboriginal_Syllabics)': [205, 220], 'Armenian': [799, 839], 'Anglo-Saxon_Futhorc': [411, 439], 'Tifinagh': [263, 317], 'Balinese': [775, 798], 'Braille': [359, 384], 'Greek': [857, 880], 'Tagalog': [840, 856], 'N_Ko': [130, 162], 'Blackfoot_(Canadian_Aboriginal_Syllabics)': [761, 774], 'Arcadian': [57, 82], 'Malay_(Jawi_-_Arabic)': [721, 760], 'Burmese_(Myanmar)': [0, 33], 'Hebrew': [163, 184], 'Syriac_(Estrangelo)': [34, 56]}\n"
     ]
    }
   ],
   "source": [
    "class Siamese_Loader:\n",
    "    def __init__(self,path):\n",
    "        self.data = {}\n",
    "        self.classes = {}\n",
    "        with open(os.path.join(path,\"train.pickle\"), \"rb\") as f:\n",
    "            (X,c) = pickle.load(f)\n",
    "            self.data[\"train\"] = X\n",
    "            self.classes[\"train\"] = c\n",
    "            print(c)\n",
    "        with open(os.path.join(path,\"val.pickle\"), \"rb\") as f:\n",
    "            (X,c) = pickle.load(f)\n",
    "            self.data[\"val\"] = X\n",
    "            self.classes[\"val\"] = c\n",
    "        self.n_classes,self.n_examples,self.w,self.h = self.data['train'].shape\n",
    "#         n_classes: ennyi betu van a tanulo adatban (964)\n",
    "#         n_examples: egy betuhoz 20 minta tartozik\n",
    "#         w: a kepek szelessege (105)\n",
    "#         h: a kepek magassaga (105)\n",
    "        self.n_val,self.n_ex,_,_ = self.data['val'].shape\n",
    "    \n",
    "    '''Oszlopvektorokat alakit ki a kepekbol\n",
    "    - a parok masodik fele megegyezo osztalybol az elso fele kulonbozo osztalybol szarmazik\n",
    "    '''\n",
    "    def get_batch(self,n,s='train'):\n",
    "        X=self.data[s]\n",
    "        classes=rng.choice(self.n_classes,size=(n,),replace=False)\n",
    "        pairs=[np.zeros((n,self.h,self.w,1)) for i in range(2)]\n",
    "        targets=np.zeros((n,))\n",
    "        targets[n//2:] = 1\n",
    "        for i in range(n):\n",
    "            class_ = classes[i]\n",
    "            idx_1 = rng.randint(0,self.n_examples)\n",
    "            pairs[0][i,:,:,:] = X[class_,idx_1].reshape(self.w,self.h,1)\n",
    "            idx_2 = rng.randint(0,self.n_examples)\n",
    "            class_2 = class_ if i >= n//2 else (class_ + rng.randint(1,self.n_classes)) % self.n_classes \n",
    "            pairs[1][i,:,:,:] = X[class_2,idx_2].reshape(self.w,self.h,1)\n",
    "        return pairs, targets\n",
    "    \n",
    "    def do_oneshot_task(self, N,s='val', language=None):\n",
    "        X = self.data[s]\n",
    "        n_classes,n_examples = X.shape[0],X.shape[1]\n",
    "        if language is None:\n",
    "            classes = rng.choice(range(n_classes),size=(N,),replace=False)\n",
    "            '''Nem biztos, hogy kell de szerintem igy a jo'''\n",
    "            #             start_idx,end_idx = self.classes[s]\n",
    "#             start_idx = 0\n",
    "#             end_idx = self.classes[n].shape[0]\n",
    "            indicies = rng.randint(0,self.n_examples,size=(N,))\n",
    "        else:\n",
    "            start_idx,end_idx = self.classes[s][language]\n",
    "            try:\n",
    "                classes = rng.choice(range(start_idx,end_idx),size=(N,),replace=False)\n",
    "            except ValueError:\n",
    "                    print(\"This language doesn't have enough characters for that one-shot task\")\n",
    "            indicies = rng.randint(0,self.n_examples,size=(N,))\n",
    "        true_class = classes[0]\n",
    "        ex1,ex2 = rng.choice(n_examples,size=(2,),replace=False)\n",
    "#         A teszt kepet lemasoljuk sokszor, hogy a siamese halo egyik agara mindig ezt tegyuk \n",
    "        test_image = np.asarray([X[true_class,ex1,:,:]]*N).reshape(N,self.w,self.h,1)\n",
    "        support_set = X[classes,indicies,:,:]\n",
    "        support_set[0,:,:] = X[true_class,ex2]\n",
    "        support_set = support_set.reshape(N,self.w,self.h,1)\n",
    "        targets = np.zeros((N,))\n",
    "        targets[0] = 1\n",
    "        targets,test_image,support_set=shuffle(targets,test_image,support_set)\n",
    "        pairs = [test_image,support_set]\n",
    "        return pairs, targets\n",
    "    \n",
    "    '''Az N azt mondja meg, hogy egy kepet hany masikkal vizsgalunk meg'''\n",
    "    '''A k azt mondja meg, hogy hanyszor teszteljuk a halot \"hany one shot feladatot vegezzunk el\"'''\n",
    "    def test_oneshot(self,model,N,k,s='val',verbose=0):\n",
    "        n_correct = 0\n",
    "        if verbose:\n",
    "            print(\"Evaluating model on {} unique {} way one-shot learning tasks ...\".format(k,N))\n",
    "        for i in range(k):\n",
    "            inputs,targets = self.do_oneshot_task(N,s,language=None)\n",
    "            probs = model.predict(inputs)\n",
    "            if np.argmax(probs) == np.argmax(targets):\n",
    "                n_correct+=1\n",
    "        percent_correct = (100*n_correct / k)\n",
    "        if verbose:\n",
    "            print(\"Got an avarage of {}% {} way one-shot learning accuracy\".format(percent_correct,N))\n",
    "        return percent_correct\n",
    "\n",
    "loader = Siamese_Loader(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy so far with this setup: 0\n"
     ]
    }
   ],
   "source": [
    "evaluate_every = 25\n",
    "batch_size = 64\n",
    "N_way = 50\n",
    "n_val = 250\n",
    "\n",
    "best_acc_str = 'BEST_ACC' + str(batch_size) + '_' + str(N_way) + '_' + str(n_val)\n",
    "best_acc = load_best_acc(best_acc_str)\n",
    "print(\"The best accuracy so far with this setup: {}\".format(best_acc))\n",
    "result_number = load_obj('result_number') + 1\n",
    "results_detail = load_obj('results_detail')\n",
    "results_detail[str(best_acc_str + '_' + str(result_number))] = {'%': [], 'iteration': [], 'loss': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST_ACC64_50_250\n",
      "{'BEST_ACC32_25_550_1': {'%': [17, 2, 8, 13, 17, 18, 21, 26, 25, 31, 30, 35, 30, 33, 29, 13, 16, 17, 19, 22, 20, 27, 19, 25, 30, 26, 30, 20, 28, 33, 29, 30, 29, 30, 28, 22, 31, 33, 28, 27, 38, 29, 33, 34, 36, 27, 30, 38, 33, 29, 32, 37, 36, 38, 28, 37, 36, 30, 41, 33, 42, 36, 37, 42, 42, 32, 35, 31, 41, 45, 36, 43, 44, 47, 43, 42, 44, 50, 46, 46, 46, 44, 46, 49, 45, 44, 51, 50, 52, 48, 48, 49, 49, 54, 52, 51, 52, 47, 53, 44, 53], 'iteration': [0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375, 400, 425, 450, 475, 500, 525, 550, 575, 600, 625, 650, 675, 700, 725, 750, 775, 800, 825, 850, 875, 900, 925, 950, 975, 1000, 1025, 1050, 1075, 1100, 1125, 1150, 1175, 1200, 1225, 1250, 1275, 1300, 1325, 1350, 1375, 1400, 1425, 1450, 1475, 1500, 1525, 1550, 1575, 1600, 1625, 1650, 1675, 1700, 1725, 1750, 1775, 1800, 1825, 1850, 1875, 1900, 1925, 1950, 1975, 2000, 2025, 2050, 2075, 2100, 2125, 2150, 2175, 2200, 2225, 2250, 2275, 2300, 2325, 2350, 2375, 2400, 2425, 2450, 2475, 2500], 'loss': [8.34, 0.0, 0.0, 0.0, 6.3, 5.89, 5.48, 5.15, 0.0, 4.55, 0.0, 4.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.26, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.94, 0.0, 0.89, 0.0, 0.0, 0.88, 0.86, 0.0, 0.0, 0.0, 0.0, 0.81, 0.0, 0.0, 0.0, 0.79, 0.0, 0.0, 0.0, 0.76, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.68, 0.0, 0.66, 0.0, 0.0, 0.0, 0.0, 0.53]}, 'BEST_ACC32_25_50_3': {'%': [14, 10, 18, 22, 12, 32, 18, 26, 30, 40, 22, 30, 32, 22, 34, 34, 30, 32, 26, 32, 34, 28, 14, 6, 16, 16, 22, 30, 16, 10, 30, 22, 26, 28, 26, 30, 20, 44, 34, 32], 'iteration': [0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375, 400, 425, 450, 475, 500, 525, 550, 575, 600, 625, 650, 675, 700, 725, 750, 775, 800, 825, 850, 875, 900, 925, 950, 975], 'loss': [8.3494492, 7.7579122, 7.2753434, 6.8041544, 6.3386841, 5.9444389, 5.5667057, 5.2193561, 4.8931885, 4.5966253, 4.3315191, 4.0797596, 3.8477678, 3.6422691, 3.449137, 3.2769439, 3.1137042, 2.9742994, 2.8298364, 2.7098086, 2.5802159, 2.4393063, 2.2464805, 2.1726422, 2.0031068, 2.1353836, 1.8619208, 1.8831431, 1.8172345, 1.6674182, 1.6726416, 1.5505798, 1.5202116, 1.5936472, 1.4165554, 1.4620322, 1.4579687, 1.26027, 1.4207015, 1.4425039]}, 'BEST_ACC64_50_250_4': {'iteration': [], '%': [], 'loss': []}, 'BEST_ACC64_25_50_0': {'%': [20, 22, 22, 32, 44, 48, 42, 52, 38, 34, 42, 50, 40, 44, 36, 44, 52, 44, 58, 56, 58, 52, 54, 52, 56, 52, 48, 64, 60, 58, 50, 46, 58, 50, 54, 42, 56, 72, 56, 54, 54, 58, 54, 66, 58, 70, 70, 62, 60, 64, 58], 'iteration': [0, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240, 260, 280, 300, 320, 340, 360, 380, 400, 420, 440, 460, 480, 500, 520, 540, 560, 580, 600, 620, 640, 660, 680, 700, 720, 740, 760, 780, 800, 820, 840, 860, 880, 900, 920, 940, 960, 980, 1000], 'loss': [3.48, 3.21, 3.15, 2.81, 2.78, 2.62, 2.44, 2.36, 2.26, 2.23, 2.09, 2.03, 1.82, 1.86, 1.7, 1.68, 1.6, 1.56, 1.49, 1.52, 1.46, 1.48, 1.37, 1.37, 1.33, 1.24, 1.37, 1.34, 1.25, 1.13, 1.13, 1.02, 1.08, 1.12, 1.07, 1.1, 1.06, 0.92, 1.11, 1.06, 0.99, 1.18, 0.94, 1.11, 0.94, 0.92, 0.82, 0.84, 0.81, 0.76, 0.74]}, 'BEST_ACC64_25_50_2': {'iteration': [0, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240, 260, 280, 300, 320, 340, 360, 380, 400, 420, 440, 460, 480, 500, 520, 540, 560, 580, 600, 620, 640, 660, 680, 700, 720, 740, 760, 780, 800, 820, 840, 860, 880, 900, 920, 940, 960, 980, 1000], '%': [10, 10, 10, 10, 20, 18, 28, 22, 26, 24, 30, 36, 26, 16, 24, 20, 24, 22, 40, 32, 28, 44, 42, 26, 44, 40, 34, 44, 28, 36, 24, 38, 44, 36, 44, 32, 28, 46, 32, 42, 30, 30, 40, 34, 34, 46, 52, 36, 40, 46, 42], 'loss': [8.352067, 7.7868128, 7.3221712, 6.8599815, 6.4291697, 6.0116429, 5.516819, 5.0880198, 4.7825632, 4.4721465, 4.212008, 4.113502, 3.7582889, 3.7420616, 3.5698805, 3.3905811, 3.1256938, 3.0147281, 2.9795046, 2.8834274, 2.6566541, 2.7174778, 2.5034993, 2.5461276, 2.4600236, 2.3110237, 2.2037516, 2.065053, 2.0950966, 2.0054622, 1.9232563, 2.0633655, 1.7797184, 1.8431684, 1.7206268, 1.6701275, 1.7261727, 1.6389768, 1.6339952, 1.5700221, 1.5168431, 1.4916968, 1.3947408, 1.4623547, 1.5761369, 1.3453197, 1.28119, 1.3314829, 1.2169025, 1.2529819, 1.2139893]}}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(best_acc_str)\n",
    "print(results_detail)\n",
    "print(result_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 19% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 0, training loss: 1.19,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 19% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 25, training loss: 1.29,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 14% 50 way one-shot learning accuracy\n",
      "Iteration 50, training loss: 1.21,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 14% 50 way one-shot learning accuracy\n",
      "Iteration 75, training loss: 1.15,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 20% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 100, training loss: 1.11,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 15% 50 way one-shot learning accuracy\n",
      "Iteration 125, training loss: 1.00,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 20% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 150, training loss: 1.00,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 19% 50 way one-shot learning accuracy\n",
      "Iteration 175, training loss: 0.97,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 21% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 200, training loss: 1.05,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 21% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 225, training loss: 1.06,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 24% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 250, training loss: 0.93,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 24% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 275, training loss: 0.93,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 26% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 300, training loss: 1.09,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 19% 50 way one-shot learning accuracy\n",
      "Iteration 325, training loss: 0.92,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 23% 50 way one-shot learning accuracy\n",
      "Iteration 350, training loss: 0.93,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 22% 50 way one-shot learning accuracy\n",
      "Iteration 375, training loss: 0.80,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 23% 50 way one-shot learning accuracy\n",
      "Iteration 400, training loss: 0.79,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 23% 50 way one-shot learning accuracy\n",
      "Iteration 425, training loss: 0.89,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 23% 50 way one-shot learning accuracy\n",
      "Iteration 450, training loss: 0.81,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 27% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 475, training loss: 0.81,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 26% 50 way one-shot learning accuracy\n",
      "Iteration 500, training loss: 0.79,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 22% 50 way one-shot learning accuracy\n",
      "Iteration 525, training loss: 0.76,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 16% 50 way one-shot learning accuracy\n",
      "Iteration 550, training loss: 0.90,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 25% 50 way one-shot learning accuracy\n",
      "Iteration 575, training loss: 0.75,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 29% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 600, training loss: 0.76,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 24% 50 way one-shot learning accuracy\n",
      "Iteration 625, training loss: 0.88,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 26% 50 way one-shot learning accuracy\n",
      "Iteration 650, training loss: 0.64,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 32% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 675, training loss: 0.66,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 27% 50 way one-shot learning accuracy\n",
      "Iteration 700, training loss: 0.71,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 30% 50 way one-shot learning accuracy\n",
      "Iteration 725, training loss: 0.71,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 36% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 750, training loss: 0.69,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 28% 50 way one-shot learning accuracy\n",
      "Iteration 775, training loss: 0.70,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 32% 50 way one-shot learning accuracy\n",
      "Iteration 800, training loss: 0.58,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 30% 50 way one-shot learning accuracy\n",
      "Iteration 825, training loss: 0.68,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 31% 50 way one-shot learning accuracy\n",
      "Iteration 850, training loss: 0.66,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 30% 50 way one-shot learning accuracy\n",
      "Iteration 875, training loss: 0.57,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 34% 50 way one-shot learning accuracy\n",
      "Iteration 900, training loss: 0.62,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 31% 50 way one-shot learning accuracy\n",
      "Iteration 925, training loss: 0.73,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 32% 50 way one-shot learning accuracy\n",
      "Iteration 950, training loss: 0.49,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 36% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 975, training loss: 0.58,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 34% 50 way one-shot learning accuracy\n",
      "Iteration 1000, training loss: 0.71,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 30% 50 way one-shot learning accuracy\n",
      "Iteration 1025, training loss: 0.69,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 34% 50 way one-shot learning accuracy\n",
      "Iteration 1050, training loss: 0.49,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 39% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 1075, training loss: 0.62,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 38% 50 way one-shot learning accuracy\n",
      "Iteration 1100, training loss: 0.51,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 35% 50 way one-shot learning accuracy\n",
      "Iteration 1125, training loss: 0.54,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 29% 50 way one-shot learning accuracy\n",
      "Iteration 1150, training loss: 0.63,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 42% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 1175, training loss: 0.53,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 45% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 1200, training loss: 0.45,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 32% 50 way one-shot learning accuracy\n",
      "Iteration 1225, training loss: 0.61,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 41% 50 way one-shot learning accuracy\n",
      "Iteration 1250, training loss: 0.47,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 40% 50 way one-shot learning accuracy\n",
      "Iteration 1275, training loss: 0.52,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 42% 50 way one-shot learning accuracy\n",
      "Iteration 1300, training loss: 0.56,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 39% 50 way one-shot learning accuracy\n",
      "Iteration 1325, training loss: 0.50,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 42% 50 way one-shot learning accuracy\n",
      "Iteration 1350, training loss: 0.54,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 34% 50 way one-shot learning accuracy\n",
      "Iteration 1375, training loss: 0.52,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 47% 50 way one-shot learning accuracy\n",
      "SAVING...\n",
      "Iteration 1400, training loss: 0.42,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 39% 50 way one-shot learning accuracy\n",
      "Iteration 1425, training loss: 0.49,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 42% 50 way one-shot learning accuracy\n",
      "Iteration 1450, training loss: 0.51,\n",
      "Evaluating model on 250 unique 50 way one-shot learning tasks ...\n",
      "Got an avarage of 38% 50 way one-shot learning accuracy\n",
      "Iteration 1475, training loss: 0.57,\n"
     ]
    }
   ],
   "source": [
    "# siamese_net.load_weights(\"\" + best_acc_str)\n",
    "\n",
    "for i in range(0, 1500):\n",
    "    (inputs,targets) = loader.get_batch(batch_size)\n",
    "    loss = siamese_net.train_on_batch(inputs,targets)\n",
    "    if i % evaluate_every == 0:\n",
    "        val_acc = loader.test_oneshot(siamese_net,N_way,n_val,verbose=True)\n",
    "        results_detail[best_acc_str + '_' + str(result_number)]['%'].append(val_acc)\n",
    "        results_detail[best_acc_str + '_' + str(result_number)]['iteration'].append(i)\n",
    "        results_detail[best_acc_str + '_' + str(result_number)]['loss'].append(loss)\n",
    "        if val_acc >= best_acc:\n",
    "            print(\"SAVING...\")\n",
    "            siamese_net.save('' + best_acc_str) \n",
    "            best_acc = val_acc\n",
    "        print(\"Iteration {}, training loss: {:.2f},\".format(i,loss))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BEST_ACC32_25_550_1': {'%': [17, 2, 8, 13, 17, 18, 21, 26, 25, 31, 30, 35, 30, 33, 29, 13, 16, 17, 19, 22, 20, 27, 19, 25, 30, 26, 30, 20, 28, 33, 29, 30, 29, 30, 28, 22, 31, 33, 28, 27, 38, 29, 33, 34, 36, 27, 30, 38, 33, 29, 32, 37, 36, 38, 28, 37, 36, 30, 41, 33, 42, 36, 37, 42, 42, 32, 35, 31, 41, 45, 36, 43, 44, 47, 43, 42, 44, 50, 46, 46, 46, 44, 46, 49, 45, 44, 51, 50, 52, 48, 48, 49, 49, 54, 52, 51, 52, 47, 53, 44, 53], 'iteration': [0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375, 400, 425, 450, 475, 500, 525, 550, 575, 600, 625, 650, 675, 700, 725, 750, 775, 800, 825, 850, 875, 900, 925, 950, 975, 1000, 1025, 1050, 1075, 1100, 1125, 1150, 1175, 1200, 1225, 1250, 1275, 1300, 1325, 1350, 1375, 1400, 1425, 1450, 1475, 1500, 1525, 1550, 1575, 1600, 1625, 1650, 1675, 1700, 1725, 1750, 1775, 1800, 1825, 1850, 1875, 1900, 1925, 1950, 1975, 2000, 2025, 2050, 2075, 2100, 2125, 2150, 2175, 2200, 2225, 2250, 2275, 2300, 2325, 2350, 2375, 2400, 2425, 2450, 2475, 2500], 'loss': [8.34, 0.0, 0.0, 0.0, 6.3, 5.89, 5.48, 5.15, 0.0, 4.55, 0.0, 4.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.26, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.94, 0.0, 0.89, 0.0, 0.0, 0.88, 0.86, 0.0, 0.0, 0.0, 0.0, 0.81, 0.0, 0.0, 0.0, 0.79, 0.0, 0.0, 0.0, 0.76, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.68, 0.0, 0.66, 0.0, 0.0, 0.0, 0.0, 0.53]}, 'BEST_ACC32_25_50_3': {'%': [14, 10, 18, 22, 12, 32, 18, 26, 30, 40, 22, 30, 32, 22, 34, 34, 30, 32, 26, 32, 34, 28, 14, 6, 16, 16, 22, 30, 16, 10, 30, 22, 26, 28, 26, 30, 20, 44, 34, 32], 'iteration': [0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375, 400, 425, 450, 475, 500, 525, 550, 575, 600, 625, 650, 675, 700, 725, 750, 775, 800, 825, 850, 875, 900, 925, 950, 975], 'loss': [8.3494492, 7.7579122, 7.2753434, 6.8041544, 6.3386841, 5.9444389, 5.5667057, 5.2193561, 4.8931885, 4.5966253, 4.3315191, 4.0797596, 3.8477678, 3.6422691, 3.449137, 3.2769439, 3.1137042, 2.9742994, 2.8298364, 2.7098086, 2.5802159, 2.4393063, 2.2464805, 2.1726422, 2.0031068, 2.1353836, 1.8619208, 1.8831431, 1.8172345, 1.6674182, 1.6726416, 1.5505798, 1.5202116, 1.5936472, 1.4165554, 1.4620322, 1.4579687, 1.26027, 1.4207015, 1.4425039]}, 'BEST_ACC64_50_250_4': {'iteration': [0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375, 400, 425, 450, 475, 500, 525, 550, 575, 600, 625, 650, 675, 700, 725, 750, 775, 800, 825, 850, 875, 900, 925, 950, 975, 1000, 1025, 1050, 1075, 1100, 1125, 1150, 1175, 1200, 1225, 1250, 1275, 1300, 1325, 1350, 1375, 1400, 1425, 1450, 1475], '%': [19, 19, 14, 14, 20, 15, 20, 19, 21, 21, 24, 24, 26, 19, 23, 22, 23, 23, 23, 27, 26, 22, 16, 25, 29, 24, 26, 32, 27, 30, 36, 28, 32, 30, 31, 30, 34, 31, 32, 36, 34, 30, 34, 39, 38, 35, 29, 42, 45, 32, 41, 40, 42, 39, 42, 34, 47, 39, 42, 38], 'loss': [1.1910208, 1.2949412, 1.207395, 1.1499548, 1.1113564, 0.99563944, 0.99520063, 0.96637619, 1.0489149, 1.064666, 0.9302727, 0.92986989, 1.0860773, 0.9192152, 0.9272815, 0.80138391, 0.78812951, 0.89489949, 0.81233001, 0.81414074, 0.7942366, 0.75719345, 0.89501339, 0.74989867, 0.75584012, 0.8755219, 0.64096254, 0.66316086, 0.70502549, 0.70688736, 0.68606353, 0.70428586, 0.58466029, 0.67587525, 0.66419512, 0.57429695, 0.61503953, 0.72744715, 0.4916909, 0.58230573, 0.71005678, 0.68975294, 0.48664725, 0.615237, 0.50571632, 0.53665709, 0.62756389, 0.5321604, 0.44673881, 0.6060456, 0.47389823, 0.52022588, 0.56279349, 0.49628061, 0.53957313, 0.5233838, 0.41711885, 0.48524761, 0.50831676, 0.56501907]}, 'BEST_ACC64_25_50_0': {'%': [20, 22, 22, 32, 44, 48, 42, 52, 38, 34, 42, 50, 40, 44, 36, 44, 52, 44, 58, 56, 58, 52, 54, 52, 56, 52, 48, 64, 60, 58, 50, 46, 58, 50, 54, 42, 56, 72, 56, 54, 54, 58, 54, 66, 58, 70, 70, 62, 60, 64, 58], 'iteration': [0, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240, 260, 280, 300, 320, 340, 360, 380, 400, 420, 440, 460, 480, 500, 520, 540, 560, 580, 600, 620, 640, 660, 680, 700, 720, 740, 760, 780, 800, 820, 840, 860, 880, 900, 920, 940, 960, 980, 1000], 'loss': [3.48, 3.21, 3.15, 2.81, 2.78, 2.62, 2.44, 2.36, 2.26, 2.23, 2.09, 2.03, 1.82, 1.86, 1.7, 1.68, 1.6, 1.56, 1.49, 1.52, 1.46, 1.48, 1.37, 1.37, 1.33, 1.24, 1.37, 1.34, 1.25, 1.13, 1.13, 1.02, 1.08, 1.12, 1.07, 1.1, 1.06, 0.92, 1.11, 1.06, 0.99, 1.18, 0.94, 1.11, 0.94, 0.92, 0.82, 0.84, 0.81, 0.76, 0.74]}, 'BEST_ACC64_25_50_2': {'iteration': [0, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240, 260, 280, 300, 320, 340, 360, 380, 400, 420, 440, 460, 480, 500, 520, 540, 560, 580, 600, 620, 640, 660, 680, 700, 720, 740, 760, 780, 800, 820, 840, 860, 880, 900, 920, 940, 960, 980, 1000], '%': [10, 10, 10, 10, 20, 18, 28, 22, 26, 24, 30, 36, 26, 16, 24, 20, 24, 22, 40, 32, 28, 44, 42, 26, 44, 40, 34, 44, 28, 36, 24, 38, 44, 36, 44, 32, 28, 46, 32, 42, 30, 30, 40, 34, 34, 46, 52, 36, 40, 46, 42], 'loss': [8.352067, 7.7868128, 7.3221712, 6.8599815, 6.4291697, 6.0116429, 5.516819, 5.0880198, 4.7825632, 4.4721465, 4.212008, 4.113502, 3.7582889, 3.7420616, 3.5698805, 3.3905811, 3.1256938, 3.0147281, 2.9795046, 2.8834274, 2.6566541, 2.7174778, 2.5034993, 2.5461276, 2.4600236, 2.3110237, 2.2037516, 2.065053, 2.0950966, 2.0054622, 1.9232563, 2.0633655, 1.7797184, 1.8431684, 1.7206268, 1.6701275, 1.7261727, 1.6389768, 1.6339952, 1.5700221, 1.5168431, 1.4916968, 1.3947408, 1.4623547, 1.5761369, 1.3453197, 1.28119, 1.3314829, 1.2169025, 1.2529819, 1.2139893]}}\n"
     ]
    }
   ],
   "source": [
    "print(results_detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dropout: https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "#### minibatch: https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run only if you want to SAVE the learning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BEST_ACC64_25_50: 72\\n', 'BEST_ACC32_25_550: 54\\n', 'BEST_ACC32_25_50: 44\\n', 'BEST_ACC64_50_250: 47\\n']\n"
     ]
    }
   ],
   "source": [
    "save_obj(results_detail, 'results_detail')\n",
    "save_obj(result_number, 'result_number')\n",
    "save_best_acc(best_acc_str, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyXH/P5U35FDImlrrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b3a5090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.plot(BEST_ACC64_25_50_results['iteration'], BEST_ACC64_25_50_results['%'])\n",
    "plt.ylabel('% Accuracy')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can be deleted later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plain_text = \"\"\"\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 20% 25 way one-shot learning accuracy\n",
    "saving...\n",
    "Iteration 0, training loss: 3.48,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 22% 25 way one-shot learning accuracy\n",
    "saving...\n",
    "Iteration 20, training loss: 3.21,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 22% 25 way one-shot learning accuracy\n",
    "saving...\n",
    "Iteration 40, training loss: 3.15,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 32% 25 way one-shot learning accuracy\n",
    "saving...\n",
    "Iteration 60, training loss: 2.81,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 44% 25 way one-shot learning accuracy\n",
    "saving...\n",
    "Iteration 80, training loss: 2.78,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 48% 25 way one-shot learning accuracy\n",
    "saving...\n",
    "Iteration 100, training loss: 2.62,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 42% 25 way one-shot learning accuracy\n",
    "Iteration 120, training loss: 2.44,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 52% 25 way one-shot learning accuracy\n",
    "saving...\n",
    "Iteration 140, training loss: 2.36,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 38% 25 way one-shot learning accuracy\n",
    "Iteration 160, training loss: 2.26,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 34% 25 way one-shot learning accuracy\n",
    "Iteration 180, training loss: 2.23,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 42% 25 way one-shot learning accuracy\n",
    "Iteration 200, training loss: 2.09,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 50% 25 way one-shot learning accuracy\n",
    "Iteration 220, training loss: 2.03,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 40% 25 way one-shot learning accuracy\n",
    "Iteration 240, training loss: 1.82,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 44% 25 way one-shot learning accuracy\n",
    "Iteration 260, training loss: 1.86,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 36% 25 way one-shot learning accuracy\n",
    "Iteration 280, training loss: 1.70,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 44% 25 way one-shot learning accuracy\n",
    "Iteration 300, training loss: 1.68,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 52% 25 way one-shot learning accuracy\n",
    "saving...\n",
    "Iteration 320, training loss: 1.60,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 44% 25 way one-shot learning accuracy\n",
    "Iteration 340, training loss: 1.56,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 58% 25 way one-shot learning accuracy\n",
    "saving...\n",
    "Iteration 360, training loss: 1.49,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 56% 25 way one-shot learning accuracy\n",
    "Iteration 380, training loss: 1.52,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 58% 25 way one-shot learning accuracy\n",
    "saving...\n",
    "Iteration 400, training loss: 1.46,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 52% 25 way one-shot learning accuracy\n",
    "Iteration 420, training loss: 1.48,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 54% 25 way one-shot learning accuracy\n",
    "Iteration 440, training loss: 1.37,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 52% 25 way one-shot learning accuracy\n",
    "Iteration 460, training loss: 1.37,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 56% 25 way one-shot learning accuracy\n",
    "Iteration 480, training loss: 1.33,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 52% 25 way one-shot learning accuracy\n",
    "Iteration 500, training loss: 1.24,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 48% 25 way one-shot learning accuracy\n",
    "Iteration 520, training loss: 1.37,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 64% 25 way one-shot learning accuracy\n",
    "saving...\n",
    "Iteration 540, training loss: 1.34,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 60% 25 way one-shot learning accuracy\n",
    "Iteration 560, training loss: 1.25,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 58% 25 way one-shot learning accuracy\n",
    "Iteration 580, training loss: 1.13,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 50% 25 way one-shot learning accuracy\n",
    "Iteration 600, training loss: 1.13,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 46% 25 way one-shot learning accuracy\n",
    "Iteration 620, training loss: 1.02,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 58% 25 way one-shot learning accuracy\n",
    "Iteration 640, training loss: 1.08,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 50% 25 way one-shot learning accuracy\n",
    "Iteration 660, training loss: 1.12,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 54% 25 way one-shot learning accuracy\n",
    "Iteration 680, training loss: 1.07,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 42% 25 way one-shot learning accuracy\n",
    "Iteration 700, training loss: 1.10,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 56% 25 way one-shot learning accuracy\n",
    "Iteration 720, training loss: 1.06,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 72% 25 way one-shot learning accuracy\n",
    "saving...\n",
    "Iteration 740, training loss: 0.92,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 56% 25 way one-shot learning accuracy\n",
    "Iteration 760, training loss: 1.11,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 54% 25 way one-shot learning accuracy\n",
    "Iteration 780, training loss: 1.06,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 54% 25 way one-shot learning accuracy\n",
    "Iteration 800, training loss: 0.99,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 58% 25 way one-shot learning accuracy\n",
    "Iteration 820, training loss: 1.18,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 54% 25 way one-shot learning accuracy\n",
    "Iteration 840, training loss: 0.94,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 66% 25 way one-shot learning accuracy\n",
    "Iteration 860, training loss: 1.11,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 58% 25 way one-shot learning accuracy\n",
    "Iteration 880, training loss: 0.94,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 70% 25 way one-shot learning accuracy\n",
    "Iteration 900, training loss: 0.92,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 70% 25 way one-shot learning accuracy\n",
    "Iteration 920, training loss: 0.82,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 62% 25 way one-shot learning accuracy\n",
    "Iteration 940, training loss: 0.84,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 60% 25 way one-shot learning accuracy\n",
    "Iteration 960, training loss: 0.81,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 64% 25 way one-shot learning accuracy\n",
    "Iteration 980, training loss: 0.76,\n",
    "Evaluating model on 50 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 58% 25 way one-shot learning accuracy\n",
    "Iteration 1000, training loss: 0.74,\n",
    "\"\"\"\n",
    "plain_text = '''\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 17% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 0, training loss: 8.34,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 2% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 8% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 13% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 17% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 100, training loss: 6.30,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 18% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 125, training loss: 5.89,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 21% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 150, training loss: 5.48,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 26% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 175, training loss: 5.15,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 25% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 31% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 225, training loss: 4.55,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 30% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 35% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 275, training loss: 4.04,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 30% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 33% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 29% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 13% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 16% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 17% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 19% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 22% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 20% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 27% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 19% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 25% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 30% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 26% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 30% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 20% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 28% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 33% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 29% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 30% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 29% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 30% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 28% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 22% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 31% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 33% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 28% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 27% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 38% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 1000, training loss: 1.26,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 29% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 33% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 34% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 36% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 27% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 30% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 38% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 1175, training loss: 1.13,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 33% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 29% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 32% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 37% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 36% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 38% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 1325, training loss: 0.99,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 28% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 37% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 36% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 30% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 41% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 1450, training loss: 0.94,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 33% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 42% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 1500, training loss: 0.89,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 36% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 37% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 42% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 1575, training loss: 0.88,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 42% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 1600, training loss: 0.86,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 32% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 35% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 31% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 41% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 45% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 1725, training loss: 0.81,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 36% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 43% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 44% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 47% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 1825, training loss: 0.79,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 43% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 42% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 44% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 50% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 1925, training loss: 0.76,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 46% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 46% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 46% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 44% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 46% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 49% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 45% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 44% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 51% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 2150, training loss: 0.68,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 50% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 52% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 2200, training loss: 0.66,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 48% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 48% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 49% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 49% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 54% 25 way one-shot learning accuracy\n",
    "SAVING...\n",
    "Iteration 2325, training loss: 0.53,\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 52% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 51% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 52% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 47% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 53% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 44% 25 way one-shot learning accuracy\n",
    "Evaluating model on 550 unique 25 way one-shot learning tasks ...\n",
    "Got an avarage of 53% 25 way one-shot learning accuracy\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "<type 'dict'>\n",
      "{'BEST_ACC32_25_550_2': {'iteration': [], '%': [], 'loss': []}, 'BEST_ACC64_25_50_0': {'iteration': [0, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240, 260, 280, 300, 320, 340, 360, 380, 400, 420, 440, 460, 480, 500, 520, 540, 560, 580, 600, 620, 640, 660, 680, 700, 720, 740, 760, 780, 800, 820, 840, 860, 880, 900, 920, 940, 960, 980, 1000], '%': [20, 22, 22, 32, 44, 48, 42, 52, 38, 34, 42, 50, 40, 44, 36, 44, 52, 44, 58, 56, 58, 52, 54, 52, 56, 52, 48, 64, 60, 58, 50, 46, 58, 50, 54, 42, 56, 72, 56, 54, 54, 58, 54, 66, 58, 70, 70, 62, 60, 64, 58], 'loss': [3.48, 3.21, 3.15, 2.81, 2.78, 2.62, 2.44, 2.36, 2.26, 2.23, 2.09, 2.03, 1.82, 1.86, 1.7, 1.68, 1.6, 1.56, 1.49, 1.52, 1.46, 1.48, 1.37, 1.37, 1.33, 1.24, 1.37, 1.34, 1.25, 1.13, 1.13, 1.02, 1.08, 1.12, 1.07, 1.1, 1.06, 0.92, 1.11, 1.06, 0.99, 1.18, 0.94, 1.11, 0.94, 0.92, 0.82, 0.84, 0.81, 0.76, 0.74]}}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "substr = ['%','Iteration ','loss:']\n",
    "indices = {}\n",
    "indices[0] = [m.start() for m in re.finditer(substr[0], plain_text)]\n",
    "indices[1] = [m.start() for m in re.finditer(substr[1], plain_text)]\n",
    "indices[2] = [m.start() for m in re.finditer(substr[2], plain_text)]\n",
    "result_part = {'%': [], 'iteration': [], 'loss': []}\n",
    "for i in range(0,len(indices[0])):\n",
    "    result_part['%'].append(int(plain_text[int(indices[0][i]-2):indices[0][i]]))\n",
    "    if i == 0:\n",
    "        result_part[\"iteration\"].append(int(plain_text[int(indices[1][i]+10):int(indices[1][i]+11)]))\n",
    "    elif i < 5:\n",
    "        result_part[\"iteration\"].append(int(plain_text[int(indices[1][i]+10):int(indices[1][i]+12)]))\n",
    "    elif i < 50:\n",
    "        result_part[\"iteration\"].append(int(plain_text[int(indices[1][i]+10):int(indices[1][i]+13)]))\n",
    "    else:\n",
    "        result_part[\"iteration\"].append(int(plain_text[int(indices[1][i]+10):int(indices[1][i]+14)]))\n",
    "    result_part['loss'].append(float(plain_text[int(indices[2][i]+6):int(indices[2][i]+10)]))\n",
    "result_part\n",
    "results_detail[str(best_acc_str + '_' + str(result_number))] = result_part\n",
    "# results_detail['BEST_ACC64_25_50_0'] = BEST_ACC64_25_50_results\n",
    "print(results_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "{'BEST_ACC32_25_550_1': {'iteration': [0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375, 400, 425, 450, 475, 500, 525, 550, 575, 600, 625, 650, 675, 700, 725, 750, 775, 800, 825, 850, 875, 900, 925, 950, 975, 1000, 1025, 1050, 1075, 1100, 1125, 1150, 1175, 1200, 1225, 1250, 1275, 1300, 1325, 1350, 1375, 1400, 1425, 1450, 1475, 1500, 1525, 1550, 1575, 1600, 1625, 1650, 1675, 1700, 1725, 1750, 1775, 1800, 1825, 1850, 1875, 1900, 1925, 1950, 1975, 2000, 2025, 2050, 2075, 2100, 2125, 2150, 2175, 2200, 2225, 2250, 2275, 2300, 2325, 2350, 2375, 2400, 2425, 2450, 2475, 2500], '%': [17, 2, 8, 13, 17, 18, 21, 26, 25, 31, 30, 35, 30, 33, 29, 13, 16, 17, 19, 22, 20, 27, 19, 25, 30, 26, 30, 20, 28, 33, 29, 30, 29, 30, 28, 22, 31, 33, 28, 27, 38, 29, 33, 34, 36, 27, 30, 38, 33, 29, 32, 37, 36, 38, 28, 37, 36, 30, 41, 33, 42, 36, 37, 42, 42, 32, 35, 31, 41, 45, 36, 43, 44, 47, 43, 42, 44, 50, 46, 46, 46, 44, 46, 49, 45, 44, 51, 50, 52, 48, 48, 49, 49, 54, 52, 51, 52, 47, 53, 44, 53], 'loss': [8.34, 0.0, 0.0, 0.0, 6.3, 5.89, 5.48, 5.15, 0.0, 4.55, 0.0, 4.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.26, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.94, 0.0, 0.89, 0.0, 0.0, 0.88, 0.86, 0.0, 0.0, 0.0, 0.0, 0.81, 0.0, 0.0, 0.0, 0.79, 0.0, 0.0, 0.0, 0.76, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.68, 0.0, 0.66, 0.0, 0.0, 0.0, 0.0, 0.53]}, 'BEST_ACC32_25_550_2': {'iteration': [], '%': [], 'loss': []}, 'BEST_ACC64_25_50_0': {'iteration': [0, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240, 260, 280, 300, 320, 340, 360, 380, 400, 420, 440, 460, 480, 500, 520, 540, 560, 580, 600, 620, 640, 660, 680, 700, 720, 740, 760, 780, 800, 820, 840, 860, 880, 900, 920, 940, 960, 980, 1000], '%': [20, 22, 22, 32, 44, 48, 42, 52, 38, 34, 42, 50, 40, 44, 36, 44, 52, 44, 58, 56, 58, 52, 54, 52, 56, 52, 48, 64, 60, 58, 50, 46, 58, 50, 54, 42, 56, 72, 56, 54, 54, 58, 54, 66, 58, 70, 70, 62, 60, 64, 58], 'loss': [3.48, 3.21, 3.15, 2.81, 2.78, 2.62, 2.44, 2.36, 2.26, 2.23, 2.09, 2.03, 1.82, 1.86, 1.7, 1.68, 1.6, 1.56, 1.49, 1.52, 1.46, 1.48, 1.37, 1.37, 1.33, 1.24, 1.37, 1.34, 1.25, 1.13, 1.13, 1.02, 1.08, 1.12, 1.07, 1.1, 1.06, 0.92, 1.11, 1.06, 0.99, 1.18, 0.94, 1.11, 0.94, 0.92, 0.82, 0.84, 0.81, 0.76, 0.74]}}\n"
     ]
    }
   ],
   "source": [
    "substr = ['%','Iteration ','loss:']\n",
    "indices = {}\n",
    "indices[0] = [m.start() for m in re.finditer(substr[0], plain_text)]\n",
    "# indices[1] = [m.start() for m in re.finditer(substr[1], plain_text)]\n",
    "indices[2] = [m.start() for m in re.finditer(substr[2], plain_text)]\n",
    "result_part = {'%': [], 'iteration': [], 'loss': []}\n",
    "print(type(result_part['%']))\n",
    "i25 = 0\n",
    "j = 0\n",
    "for i in range(0,len(indices[0])):\n",
    "    result_part['%'].append(int(plain_text[int(indices[0][i]-2):indices[0][i]]))\n",
    "    \n",
    "    result_part['iteration'].append(i25)\n",
    "    i25 += 25\n",
    "#     if i == 0:\n",
    "#         BEST_ACC64_25_50_results[\"iteration\"].append(int(plain_text[int(indices[1][i]+10):int(indices[1][i]+11)]))\n",
    "#     elif i < 5:\n",
    "#         BEST_ACC64_25_50_results[\"iteration\"].append(int(plain_text[int(indices[1][i]+10):int(indices[1][i]+12)]))\n",
    "#     elif i < 50:\n",
    "#         BEST_ACC64_25_50_results[\"iteration\"].append(int(plain_text[int(indices[1][i]+10):int(indices[1][i]+13)]))\n",
    "#     else:\n",
    "#         BEST_ACC64_25_50_results[\"iteration\"].append(int(plain_text[int(indices[1][i]+10):int(indices[1][i]+14)]))\n",
    "    if i < len(indices[0]) - 1 and j < len(indices[2]):\n",
    "        if indices[2][j] > indices[0][i] and indices[2][j] < indices[0][i+1]:\n",
    "            result_part['loss'].append(float(plain_text[int(indices[2][j]+6):int(indices[2][j]+10)]))\n",
    "            j += 1\n",
    "        else:\n",
    "            result_part['loss'].append(float(0))\n",
    "\n",
    "results_detail[str(best_acc_str + '_' + str(result_number))] = result_part\n",
    "# results_detail['BEST_ACC32_25_550_1'] = result_part\n",
    "print(results_detail)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
